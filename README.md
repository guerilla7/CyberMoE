# CyberMoE: A Minimal Mixture-of-Experts Demonstration
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/guerilla7/CyberMoE)

![GitHub Repo stars](https://img.shields.io/github/stars/guerilla7/CyberMoE?style=social)
![GitHub forks](https://img.shields.io/github/forks/guerilla7/CyberMoE?style=social)
![GitHub issues](https://img.shields.io/github/issues/guerilla7/CyberMoE)
![GitHub License](https://img.shields.io/github/license/guerilla7/CyberMoE)
![GitHub last commit](https://img.shields.io/github/last-commit/guerilla7/CyberMoE)
![Python](https://img.shields.io/badge/python-3.8%2B-blue)

# ğŸ“‘ Table of Contents

- [ğŸ–¼ï¸ Screenshots](#screenshots)
- [ğŸ“¦ Project Structure](#project-structure)
- [ğŸ’¡ Core Concepts Demonstrated](#core-concepts-demonstrated)
- [âš™ï¸ How It Works](#how-it-works)
- [ğŸ› ï¸ Setup](#setup)
- [ğŸš€ How to Run](#how-to-run)
- [ğŸ” Interpreting the Output](#interpreting-the-output)
- [ğŸ—‚ï¸ Data Files Required for Testing](#data-files-required-for-testing)
- [ğŸ›ï¸ Built-in RLHF Loop and Feedback Analytics](#built-in-rlhf-loop-and-feedback-analytics)
- [ğŸ“¦ Large Files and Git LFS](#large-files-and-git-lfs)
- [ğŸ§° Troubleshooting](#troubleshooting)
- [âœ… To-Do](#to-do)

---

This project provides a minimal, educational implementation of a sparse **Mixture-of-Experts (MoE)** model. It is designed to demonstrate the core concepts of MoE within a cybersecurity context, showing how different "expert" models can be used to classify security-related text.

This repository contains two main components:
1.  A command-line script (`CyberMoe.py`) that trains the model and runs inference on a predefined set of examples.
2.  An interactive web demo (`app.py`) built with Streamlit that allows you to classify your own sentences and visualize the MoE routing in real-time.

## ğŸ–¼ï¸ Screenshots

<img width="1265" height="1064" alt="Screenshot 2025-10-02 at 11 13 37â€¯PM" src="https://github.com/user-attachments/assets/926491be-7e6b-4610-89f4-fcf9efcc30bf" />
<img width="1267" height="1225" alt="Screenshot 2025-10-02 at 11 16 13â€¯PM" src="https://github.com/user-attachments/assets/47cd5b93-cbf2-4f36-ab7d-2a6b09e44fc4" />
<img width="1267" height="1083" alt="Screenshot 2025-10-02 at 11 17 11â€¯PM" src="https://github.com/user-attachments/assets/65666f4e-f49e-424d-ad35-a83a908c6d19" />
<img width="1267" height="1245" alt="Screenshot 2025-10-02 at 11 18 05â€¯PM" src="https://github.com/user-attachments/assets/99d7c305-4d06-491d-8276-34958bfaf994" />
<img width="1263" height="1317" alt="Screenshot 2025-10-02 at 11 19 46â€¯PM" src="https://github.com/user-attachments/assets/02e10235-bad3-47b9-98c7-c919defaf5bd" />
<img width="257" height="435" alt="Screenshot 2025-10-02 at 11 22 07â€¯PM" src="https://github.com/user-attachments/assets/c0196004-700f-4cef-b3fe-4bc5087b42c6" />
<img width="257" height="382" alt="Screenshot 2025-10-02 at 11 22 59â€¯PM" src="https://github.com/user-attachments/assets/59a74041-e37f-488d-8c65-3f305753f5a1" />
<img width="257" height="580" alt="Screenshot 2025-10-02 at 11 23 47â€¯PM" src="https://github.com/user-attachments/assets/564c2fcd-2684-407c-93b7-5bc8aed33ecf" />

---

<details>
<summary>ğŸ“¦ Project Structure</summary>
<!-- Full content here -->
</details>

<details>
<summary>ğŸ’¡ Core Concepts Demonstrated</summary>
<!-- Full content here -->
</details>

<details>
<summary>âš™ï¸ How It Works</summary>
<!-- Full content here -->
</details>

<details>
<summary>ğŸ› ï¸ Setup</summary>
<!-- Full content here -->
</details>

<details>
<summary>ğŸš€ How to Run</summary>
<!-- Full content here -->
</details>

<details>
<summary>ğŸ” Interpreting the Output</summary>
<!-- Full content here -->
</details>

<details>
<summary>ğŸ—‚ï¸ Data Files Required for Testing</summary>
<!-- Full content here -->
</details>

<details>
<summary>ğŸ›ï¸ Built-in RLHF Loop and Feedback Analytics</summary>
<!-- Full content here -->
</details>

<details>
<summary>ğŸ“¦ Large Files and Git LFS</summary>
<!-- Full content here -->
</details>

<details>
<summary>ğŸ§° Troubleshooting</summary>
<!-- Full content here -->
</details>

<details>
<summary>âœ… To-Do</summary>

- [ ] Improve UI/UX
- [ ] Add real-world datasets for each Expert Network
- [ ] Pluggable MoE Architecture: The ability for users to configure and use a Small Language Model of choice for their Expert Networks
- [ ] Improve and/or optimize Gating Network behavior

</details>
