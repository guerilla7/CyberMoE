# CyberMoE: A Minimal Mixture-of-Experts Demonstration
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/guerilla7/CyberMoE) 

<!-- Repo status badges -->
![GitHub Repo stars](https://img.shields.io/github/stars/guerilla7/CyberMoE?style=social)
![GitHub forks](https://img.shields.io/github/forks/guerilla7/CyberMoE?style=social)
![GitHub issues](https://img.shields.io/github/issues/guerilla7/CyberMoE)
![GitHub License](https://img.shields.io/github/license/guerilla7/CyberMoE)
![GitHub last commit](https://img.shields.io/github/last-commit/guerilla7/CyberMoE)
![Python](https://img.shields.io/badge/python-3.8%2B-blue)

This project provides a minimal, educational implementation of a sparse **Mixture-of-Experts (MoE)** model. It is designed to demonstrate the core concepts of MoE within a cybersecurity context, sh...